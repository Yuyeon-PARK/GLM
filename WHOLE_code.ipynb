{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "######### 21-2 GLM Final Project #########\n",
    "##########################################\n",
    "\n",
    "######## 1. EDA\n",
    "import math\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "\n",
    "df = pd.read_csv(\"~\\\\data1.csv\", header=0,engine='python')  \n",
    "df.info()\n",
    "df = df.loc[0:117]\n",
    "df = df[[\"날짜\",\"정책\",\"정책지수\",\"누적정책지수\",\"서울\",\"용산구\",\"광진구\",\"서대문구\",\"영등포구\",\"서초구\",\"소비자물가지수\",\"한국은행기준금리\",\"원달러환율\",\"가계대출\",\"서울시부동산심리지수\",\"서울실거래지수\",\"도심권실거래지수\",\"동북권실거래지수\",\"서북권실거래지수\",\"서남권실거래지수\",\"동남권실거래지수\"]]\n",
    "df.columns = [\"date\",\"1\",\"PI\",\"PI2\",\"Seoul\",\"Yongsan-gu\",\"Gwangjin-gu\",\"Seodaemun-gu\",\"Yeongdeungpo-gu\",\"Seocho-gu\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\",\"SI\",\"CI\",\"NEI\",\"NWI\",\"SWI\",\"SEI\"]\n",
    "df.tail()\n",
    "\n",
    "sns.set(rc={'figure.figsize':(16,10)})\n",
    "sns.boxplot(data=df.iloc[:,1:12])\n",
    "\n",
    "feature = df[[\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "\n",
    "whole1 = df[[\"Seoul\",\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "\n",
    "whole2 = df[[\"Yongsan-gu\",\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "whole3 = df[[\"Gwangjin-gu\",\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "whole4 = df[[\"Seodaemun-gu\",\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "whole5 = df[[\"Yeongdeungpo-gu\",\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "whole6 = df[[\"Seocho-gu\",\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "\n",
    "label = df[[\"Seoul\",\"Yongsan-gu\",\"Gwangjin-gu\",\"Seodaemun-gu\",\"Yeongdeungpo-gu\",\"Seocho-gu\"]]\n",
    "\n",
    "label1 = df[[\"Seoul\"]]\n",
    "label2 = df[[\"Yongsan-gu\"]]\n",
    "label3 = df[[\"Gwangjin-gu\"]]\n",
    "label4 = df[[\"Seodaemun-gu\"]]\n",
    "label5 = df[[\"Yeongdeungpo-gu\"]]\n",
    "label6 = df[[\"Seocho-gu\"]]\n",
    "\n",
    "date=df[[\"date\"]]\n",
    "\n",
    "sns.set(rc={'figure.figsize':(16,10)})\n",
    "sns.boxplot(data=label)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "label.plot()\n",
    "plt.title(\"Price of Apartment in Seoul(Won)\")\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "feature.PI.plot()\n",
    "plt.title(\"Policy Index\")\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "feature.CPI.plot()\n",
    "plt.title(\"Customer Price Index\")\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "feature.BR.plot()\n",
    "plt.title(\"Base Rate by Bank of Korea\")\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "feature.ER.plot()\n",
    "plt.title(\"Exchange Rate\")\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "feature.HL.plot()\n",
    "plt.title(\"Housing Loan\")\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "feature.EI.plot()\n",
    "plt.title(\"Real-estate Index\")\n",
    "\n",
    "import pandas_profiling\n",
    "\n",
    "#pr=df.profile_report() \n",
    "#df.profile_report()\n",
    "#pr.to_file('/Users/kimhyunmin/data.html')\n",
    "\n",
    "corr = df.corr()\n",
    "corr1 = feature.corr()\n",
    "corr2 = whole2.corr()\n",
    "corr3 = whole3.corr()\n",
    "corr4 = whole4.corr()\n",
    "corr5 = whole5.corr()\n",
    "corr6 = whole6.corr()\n",
    "\n",
    "sns.clustermap(corr, annot = True, cmap = 'RdYlBu_r', vmin = -1, vmax = 1, )\n",
    "sns.clustermap(corr1, annot = True, cmap = 'RdYlBu_r', vmin = -1, vmax = 1, )\n",
    "sns.clustermap(corr2, annot = True, cmap = 'RdYlBu_r', vmin = -1, vmax = 1, )\n",
    "sns.clustermap(corr3, annot = True, cmap = 'RdYlBu_r', vmin = -1, vmax = 1, )\n",
    "sns.clustermap(corr4, annot = True, cmap = 'RdYlBu_r', vmin = -1, vmax = 1, )\n",
    "sns.clustermap(corr5, annot = True, cmap = 'RdYlBu_r', vmin = -1, vmax = 1, )\n",
    "sns.clustermap(corr6, annot = True, cmap = 'RdYlBu_r', vmin = -1, vmax = 1, )\n",
    "\n",
    "######## 2-1. LSTM ########\n",
    "# load libraries\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, LSTM,Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# load the dataset\n",
    "df = pd.read_csv(\"~\\\\data1.csv\", header=0,engine='python') \n",
    "df = df.loc[0:117]\n",
    "df = df[[\"날짜\",\"정책\",\"정책지수\",\"누적정책지수\",\"서울\",\"용산구\",\"광진구\",\"서대문구\",\"영등포구\",\"서초구\",\"소비자물가지수\",\"한국은행기준금리\",\"원달러환율\",\"가계대출\",\"서울시부동산심리지수\",\"서울실거래지수\",\"도심권실거래지수\",\"동북권실거래지수\",\"서북권실거래지수\",\"서남권실거래지수\",\"동남권실거래지수\"]]\n",
    "df.columns = [\"date\",\"1\",\"PI\",\"PI2\",\"Seoul\",\"Yongsan-gu\",\"Gwangjin-gu\",\"Seodaemun-gu\",\"Yeongdeungpo-gu\",\"Seocho-gu\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\",\"SI\",\"CI\",\"NEI\",\"NWI\",\"SWI\",\"SEI\"]\n",
    "df.tail()\n",
    "\n",
    "feature = df[[\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "\n",
    "whole1 = df[[\"Seoul\",\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "\n",
    "whole2 = df[[\"Yongsan-gu\",\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "whole3 = df[[\"Gwangjin-gu\",\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "whole4 = df[[\"Seodaemun-gu\",\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "whole5 = df[[\"Yeongdeungpo-gu\",\"PI\",\"CPI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "whole6 = df[[\"Seocho-gu\",\"CPI\",\"PI\",\"BR\",\"ER\",\"HL\",\"EI\"]]\n",
    "\n",
    "label = df[[\"Seoul\",\"Yongsan-gu\",\"Gwangjin-gu\",\"Seodaemun-gu\",\"Yeongdeungpo-gu\",\"Seocho-gu\"]]\n",
    "\n",
    "label1 = df[[\"Seoul\"]]\n",
    "label2 = df[[\"Yongsan-gu\"]]\n",
    "label3 = df[[\"Gwangjin-gu\"]]\n",
    "label4 = df[[\"Seodaemun-gu\"]]\n",
    "label5 = df[[\"Yeongdeungpo-gu\"]]\n",
    "label6 = df[[\"Seocho-gu\"]]\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "xdata = scaler.fit_transform(feature)\n",
    "ydata = scaler.fit_transform(label2)\n",
    "wholedata=scaler.fit_transform(whole2)\n",
    "\n",
    "# Window -> X timestep back\n",
    "step_back = 1\n",
    "X_whole, Y_whole = [], []\n",
    "for i in range(len(wholedata)-step_back - 1):\n",
    "    a = wholedata[i:(i+step_back), 1:]\n",
    "    X_whole.append(a)\n",
    "    Y_whole.append(wholedata[i + step_back, 0])\n",
    "X_whole = np.array(X_whole); Y_whole = np.array(Y_whole)\n",
    "X_whole.shape\n",
    "\n",
    "# CV\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
    "import keras.backend as K \n",
    "import tensorflow as tf\n",
    "\n",
    "kfold=KFold(10)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X_whole, Y_whole):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_whole,Y_whole, test_size = 0.1)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], step_back, 6))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], step_back, 6))\n",
    "    \n",
    "    output_size=7 #feature+1\n",
    "    activ_func=\"relu\"\n",
    "    dropout=0.25\n",
    "    loss=\"mean_squared_error\" \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100,return_sequences=True,input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(150))\n",
    "    model.add(Dense(units=output_size))\n",
    "    model.add(Activation(activ_func))\n",
    "\n",
    "    model.compile(loss=loss, optimizer='adam',metrics =[\"mse\"])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=24, verbose=0,shuffle=False)\n",
    "    \n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(xdata)*0.9)+4\n",
    "x_train_dataset, x_test_dataset = xdata[0:train_size,:], xdata[train_size:,:]\n",
    "y_train_dataset, y_test_dataset = ydata[0:train_size], ydata[train_size:]\n",
    "\n",
    "# Window -> X timestep back\n",
    "step_back= 1\n",
    "X_train, Y_train = [], []\n",
    "for i in range(len(x_train_dataset)-step_back - 1):\n",
    "    a = x_train_dataset[i:(i+step_back), :]\n",
    "    X_train.append(a)\n",
    "    Y_train.append(y_train_dataset[i + step_back, 0])\n",
    "X_train = np.array(X_train); Y_train = np.array(Y_train)\n",
    "    \n",
    "X_test, Y_test = [], []\n",
    "for i in range(len(x_test_dataset)-step_back - 1):\n",
    "    a = x_test_dataset[i:(i+step_back), :]\n",
    "    X_test.append(a)\n",
    "    Y_test.append(y_test_dataset[i + step_back, 0])\n",
    "X_test = np.array(X_test); Y_test = np.array(Y_test)\n",
    "\n",
    "print(X_train.shape); print(Y_train.shape);             \n",
    "print(X_test.shape); print(Y_test.shape)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], step_back, 6))\n",
    "X_test  = np.reshape(X_test, (X_test.shape[0], step_back, 6))\n",
    "Y_train = np.reshape(Y_train, (Y_train.shape[0], 1))\n",
    "Y_test  = np.reshape(Y_test, (Y_test.shape[0], 1))\n",
    "\n",
    "# setup a LSTM network in keras\n",
    "output_size=7 #feature+1\n",
    "activ_func=\"relu\"\n",
    "dropout=0.25\n",
    "loss=\"mean_squared_error\" \n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(100,return_sequences=True,input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model_lstm.add(Dropout(dropout))\n",
    "model_lstm.add(Dense(150))\n",
    "model_lstm.add(Dense(units=output_size))\n",
    "model_lstm.add(Activation(activ_func))\n",
    "\n",
    "model_lstm.compile(loss=loss, optimizer='adam')\n",
    "model_lstm.summary()\n",
    "lstm_history = model_lstm.fit(X_train, Y_train, epochs=50, batch_size=24, validation_data=(X_test, Y_test), shuffle=False, verbose=1)\n",
    "\n",
    "# Estimate model performance\n",
    "trainScore = model_lstm.evaluate(X_train, Y_train, verbose=1)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, np.sqrt(trainScore)))\n",
    "\n",
    "# Evaluate the skill of the Trained model\n",
    "testPredict  = model_lstm.predict(X_test)\n",
    "testPredict\n",
    "testPredict.shape\n",
    "\n",
    "result1=[]\n",
    "for i in range(0,testPredict.shape[0]):\n",
    "    result1.append(testPredict[i][0])\n",
    "    i=+1\n",
    "\n",
    "result1\n",
    "\n",
    "# invert predictions\n",
    "testPredict1 = scaler.inverse_transform(result1)\n",
    "\n",
    "result_pred=[]\n",
    "for i in range(0,testPredict1.shape[0]):\n",
    "    result_pred.append(testPredict1[i][0])\n",
    "    i=+1\n",
    "\n",
    "result_pred\n",
    "\n",
    "result11=[]\n",
    "for i in range(0,len(result1)):\n",
    "    result11.append(result1[i][0])\n",
    "    i=+1\n",
    "result11=np.reshape(result11,(-1,1))\n",
    "\n",
    "#rmse\n",
    "math.sqrt(sum((result11-Y_test)**2)/6)\n",
    "\n",
    "#plt.figure(figsize=(16,9))\n",
    "plt.scatter(result11,Y_test)\n",
    "\n",
    "# plot baseline and predictions\n",
    "#plt.figure(figsize=(16,9))\n",
    "plt.plot(result_pred)\n",
    "plt.plot(label2[-6:].values)\n",
    "plt.show()\n",
    "\n",
    "######## 2-2. CNN-LSTM ########\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from torch import nn, optim\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# settings\n",
    "\n",
    "label=label2\n",
    "\n",
    "#feature.sort_index(ascending=False).reset_index(drop=True)\n",
    "#label.sort_index(ascending=False).reset_index(drop=True)\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "label_scaler = MinMaxScaler()\n",
    "\n",
    "feature = feature_scaler.fit_transform(feature)\n",
    "feature = pd.DataFrame(feature)\n",
    "\n",
    "label = label_scaler.fit_transform(label)\n",
    "label = pd.DataFrame(label)\n",
    "\n",
    "def make_dataset(df, label, window_size):\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "    for i in range(len(df) - window_size):\n",
    "        feature_list.append(np.array(df.iloc[i:i+window_size]))\n",
    "        label_list.append(np.array(label.iloc[i+window_size]))\n",
    "    return np.array(feature_list), np.array(label_list)\n",
    "\n",
    "import os\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model_path = 'model'\n",
    "filename = os.path.join(model_path, 'tmp_checkpoint.h5')\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "\n",
    "# CV\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
    "\n",
    "kfold=KFold(10)\n",
    "cvscores = []\n",
    "\n",
    "import random\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation,Dense,Flatten,LSTM,Dropout,Conv1D, MaxPooling1D, TimeDistributed\n",
    "\n",
    "for train, test in kfold.split(feature,label):\n",
    "    \n",
    "    train_feature, test_feature = feature[0:106], feature[93:106]\n",
    "    train_label, test_label = label[0:106], label[93:106]\n",
    "    pred_feature, pred_label = feature[105:118], label[105:118]\n",
    "\n",
    "    train_feature, train_label = make_dataset(train_feature,train_label, 6)\n",
    "    test_feature, test_label = make_dataset(test_feature, test_label, 6)\n",
    "    pred_feature, pred_label = make_dataset(pred_feature, pred_label, 6)\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(train_feature, train_label, test_size=0.2)\n",
    "    \n",
    "    subsequences = 2\n",
    "    timesteps = x_train.shape[1]//subsequences\n",
    "    X_train_series_sub = x_train.reshape((x_train.shape[0], subsequences, timesteps, 6)) \n",
    "    X_valid_series_sub = x_valid.reshape((x_valid.shape[0], subsequences, timesteps, 6))\n",
    "    test_feature_series_sub = test_feature.reshape((test_feature.shape[0], subsequences, timesteps, 6))\n",
    "    pred_feature_series_sub = pred_feature.reshape((pred_feature.shape[0], subsequences, timesteps, 6))\n",
    "    \n",
    "    output_size=6 \n",
    "    neurons=100 \n",
    "    loss=\"mean_squared_error\" \n",
    "    \n",
    "    model_cnn_lstm = Sequential()\n",
    "    model_cnn_lstm.add(TimeDistributed(Conv1D(filters=128, kernel_size=2, \n",
    "                                          activation='relu'),\n",
    "                                   input_shape=(None, X_train_series_sub.shape[2],\n",
    "                                                X_train_series_sub.shape[3])))\n",
    "    model_cnn_lstm.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "    model_cnn_lstm.add(TimeDistributed(Dropout((0.5))))\n",
    "    model_cnn_lstm.add(TimeDistributed(Flatten()))\n",
    "    model_cnn_lstm.add(LSTM(100, activation='relu'))\n",
    "    model_cnn_lstm.add(Dropout(0.25))\n",
    "    model_cnn_lstm.add(Dense(150))\n",
    "    model_cnn_lstm.add(Dense(1))\n",
    "    model_cnn_lstm.compile(loss=loss, optimizer=\"adam\", metrics=[\"mse\"])\n",
    "    model_cnn_lstm.fit(X_train_series_sub, y_train,\n",
    "                    epochs=50, \n",
    "                    verbose=1, \n",
    "                    validation_data=(X_valid_series_sub, y_valid), \n",
    "                    callbacks=[early_stop, checkpoint])\n",
    "    scores = model_cnn_lstm.evaluate(test_feature_series_sub, test_label, verbose=0)\n",
    "    cvscores = (scores * 100)\n",
    "    \n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))        \n",
    "\n",
    "# prediction\n",
    "test_pred = model_cnn_lstm.predict(test_feature_series_sub)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def RMSE(y_test, y_predict):\n",
    "    return np.sqrt(mean_squared_error(y_test, y_predict))\n",
    "print('RMSE : ', RMSE(test_label, test_pred)) \n",
    "\n",
    "test_pred = label_scaler.inverse_transform(test_pred)\n",
    "test_label = label_scaler.inverse_transform(test_label)\n",
    "\n",
    "pred = model_cnn_lstm.predict(pred_feature_series_sub)\n",
    "true = pred_label\n",
    "\n",
    "RMSE(true, pred)\n",
    "\n",
    "pred = label_scaler.inverse_transform(pred)\n",
    "pred\n",
    "\n",
    "true = label2.tail(6)\n",
    "true = np.array(true)\n",
    "true\n",
    "\n",
    "plt.plot(true, label = 'actual')\n",
    "plt.plot(pred, label = 'prediction')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "######## 2-3. GRU ########\n",
    "# load libraries\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "xdata = scaler.fit_transform(feature)\n",
    "ydata = scaler.fit_transform(label2)\n",
    "wholedata=scaler.fit_transform(whole2)\n",
    "\n",
    "# Window -> X timestep back\n",
    "step_back = 1\n",
    "X_whole, Y_whole = [], []\n",
    "for i in range(len(wholedata)-step_back - 1):\n",
    "    a = wholedata[i:(i+step_back), 1:]\n",
    "    X_whole.append(a)\n",
    "    Y_whole.append(wholedata[i + step_back, 0])\n",
    "X_whole = np.array(X_whole); Y_whole = np.array(Y_whole)\n",
    "X_whole.shape\n",
    "\n",
    "# CV\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
    "import keras.backend as K \n",
    "import tensorflow as tf\n",
    "\n",
    "kfold=KFold(10)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X_whole, Y_whole):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_whole, Y_whole, test_size = 0.1)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], step_back, 6))\n",
    "    X_test  = np.reshape(X_test, (X_test.shape[0], step_back, 6))\n",
    "    \n",
    "    output_size=7\n",
    "    activ_func=\"relu\"\n",
    "    dropout=0.25\n",
    "    loss=\"mean_squared_error\" \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(GRU(75, return_sequences=True,input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(GRU(units=30, return_sequences=True))\n",
    "    model.add(Dense(units=output_size,activation=activ_func))\n",
    "\n",
    "    model.compile(loss=loss, optimizer='adam',metrics =[\"mse\"])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=24, verbose=0,shuffle=False)\n",
    "    \n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(xdata)*0.9)+4\n",
    "x_train_dataset, x_test_dataset = xdata[0:train_size,:], xdata[train_size:,:]\n",
    "y_train_dataset, y_test_dataset = ydata[0:train_size], ydata[train_size:]\n",
    "\n",
    "# Window -> X timestep back\n",
    "step_back= 1\n",
    "X_train, Y_train = [], []\n",
    "for i in range(len(x_train_dataset)-step_back - 1):\n",
    "    a = x_train_dataset[i:(i+step_back), :]\n",
    "    X_train.append(a)\n",
    "    Y_train.append(y_train_dataset[i + step_back, 0])\n",
    "X_train = np.array(X_train); Y_train = np.array(Y_train)\n",
    "    \n",
    "X_test, Y_test = [], []\n",
    "for i in range(len(x_test_dataset)-step_back - 1):\n",
    "    a = x_test_dataset[i:(i+step_back), :]\n",
    "    X_test.append(a)\n",
    "    Y_test.append(y_test_dataset[i + step_back, 0])\n",
    "X_test = np.array(X_test); Y_test = np.array(Y_test)\n",
    "\n",
    "print(X_train.shape); print(Y_train.shape);             \n",
    "print(X_test.shape); print(Y_test.shape)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], step_back, 6))\n",
    "X_test  = np.reshape(X_test, (X_test.shape[0], step_back, 6))\n",
    "Y_train = np.reshape(Y_train, (Y_train.shape[0], 1))\n",
    "Y_test  = np.reshape(Y_test, (Y_test.shape[0], 1))\n",
    "\n",
    "# setup a GRU network in keras\n",
    "output_size=7\n",
    "activ_func=\"relu\"\n",
    "dropout=0.25\n",
    "loss=\"mean_squared_error\" \n",
    "\n",
    "model_gru = Sequential()\n",
    "model_gru.add(GRU(75, return_sequences=True,input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model_gru.add(Dense(10))\n",
    "model_gru.add(Dropout(0.25))\n",
    "model_gru.add(GRU(units=30, return_sequences=True))\n",
    "model_gru.add(Dense(units=output_size, activation=\"relu\"))\n",
    "\n",
    "model_gru.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model_gru.summary()\n",
    "\n",
    "gru_history = model_gru.fit(X_train, Y_train, epochs=50, batch_size=24, validation_data=(X_test, Y_test), shuffle=False, verbose=1)\n",
    "\n",
    "# Estimate model performance\n",
    "trainScore = model_gru.evaluate(X_train, Y_train, verbose=1)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, np.sqrt(trainScore)))\n",
    "\n",
    "# Evaluate the skill of the Trained model\n",
    "testPredict  = model_gru.predict(X_test)\n",
    "testPredict.shape\n",
    "\n",
    "result1=[]\n",
    "for i in range(0,testPredict.shape[0]):\n",
    "    result1.append(testPredict[i][0])\n",
    "    i=+1\n",
    "result1\n",
    "\n",
    "# invert predictions\n",
    "testPredict1 = scaler.inverse_transform(result1)\n",
    "\n",
    "result_pred=[]\n",
    "for i in range(0,testPredict1.shape[0]):\n",
    "    result_pred.append(testPredict1[i][0])\n",
    "    i=+1\n",
    "\n",
    "result_pred\n",
    "\n",
    "result11=[]\n",
    "for i in range(0,len(result1)):\n",
    "    result11.append(result1[i][0])\n",
    "    i=+1\n",
    "result11=np.reshape(result11,(-1,1))\n",
    "\n",
    "#rmse\n",
    "math.sqrt(sum((result11-Y_test)**2)/6)\n",
    "\n",
    "#plt.figure(figsize=(10,6))\n",
    "plt.scatter(result11,Y_test)\n",
    "\n",
    "# plot baseline and predictions\n",
    "#plt.figure(figsize=(10,6))\n",
    "plt.plot(result_pred)\n",
    "plt.plot(label2[-6:].values)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32d951a4fb713309e6a15ac834ff0fb502c06e5ab8683a64badeae54cb1002e2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('test3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
